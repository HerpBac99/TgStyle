#!/usr/bin/env python3
"""
FastVLM Server - –æ—Ç–¥–µ–ª—å–Ω—ã–π —Å–µ—Ä–≤–µ—Ä –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
–ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ –æ—Ç –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
"""

import sys
import json
import base64
import tempfile
import signal
import time
import logging
from logging.handlers import RotatingFileHandler
from flask import Flask, request, jsonify
from PIL import Image
import io
import os
import psutil

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
from config import Config

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –º–æ–¥—É–ª–∏ –¥–ª—è FastVLM
import torch

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º FastVLM
sys.path.append('../ml-fastvlm')
from llava.utils import disable_torch_init
from llava.conversation import conv_templates
from llava.model.builder import load_pretrained_model
from llava.mm_utils import tokenizer_image_token, process_images, get_model_name_from_path
from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN

app = Flask(__name__)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –º–æ–¥–µ–ª–∏
model = None
tokenizer = None
image_processor = None
context_len = None

# –ì–ª–æ–±–∞–ª—å–Ω–∞—è –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –¥–ª—è –ø—Ä–æ–º–ø—Ç–∞
default_prompt = None

def setup_logging():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è"""
    log_file = os.path.join(Config.LOG_DIR, 'fastvlm.log')

    # –°–æ–∑–¥–∞–µ–º —Ñ–æ—Ä–º–∞—Ç—Ç–µ—Ä
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    # –°–æ–∑–¥–∞–µ–º —Ä–æ—Ç–∏—Ä—É—é—â–∏–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫
    handler = RotatingFileHandler(
        log_file,
        maxBytes=Config.LOG_MAX_BYTES,
        backupCount=Config.LOG_BACKUP_COUNT
    )
    handler.setFormatter(formatter)

    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–æ–≥–≥–µ—Ä –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
    app.logger.addHandler(handler)
    app.logger.setLevel(getattr(logging, Config.LOG_LEVEL))

    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∫–æ—Ä–Ω–µ–≤–æ–π –ª–æ–≥–≥–µ—Ä
    root_logger = logging.getLogger()
    root_logger.addHandler(handler)
    root_logger.setLevel(getattr(logging, Config.LOG_LEVEL))

    print(f"üìù –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ: {log_file}")

def load_prompt():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–æ–º–ø—Ç –∏–∑ —Ñ–∞–π–ª–∞ prompt.md"""
    global default_prompt
    prompt_file = os.path.join(os.path.dirname(__file__), 'prompt.md')

    try:
        with open(prompt_file, 'r', encoding='utf-8') as f:
            content = f.read()

        # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –ø—Ä–æ–º–ø—Ç –º–µ–∂–¥—É ``` –±–ª–æ–∫–∞–º–∏
        import re
        prompt_match = re.search(r'```\s*(.*?)\s*```', content, re.DOTALL)
        if prompt_match:
            default_prompt = prompt_match.group(1).strip()
        else:
            # –ï—Å–ª–∏ –Ω–µ—Ç ``` –±–ª–æ–∫–æ–≤, –±–µ—Ä–µ–º –≤–µ—Å—å –∫–æ–Ω—Ç–µ–Ω—Ç
            default_prompt = content.strip()

        app.logger.info(f"‚úÖ –ü—Ä–æ–º–ø—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ —Ñ–∞–π–ª–∞: {prompt_file}")
        print(f"üìù –ü—Ä–æ–º–ø—Ç –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ —Ñ–∞–π–ª–∞: {len(default_prompt)} —Å–∏–º–≤–æ–ª–æ–≤")

    except FileNotFoundError:
        default_prompt = '–û–ø–∏—à–∏ –ø–æ–¥—Ä–æ–±–Ω–æ –∫–∞–∫–∏–µ –ø—Ä–µ–¥–º–µ—Ç—ã –æ–¥–µ–∂–¥—ã —Ç—ã –≤–∏–¥–∏—à—å –Ω–∞ —ç—Ç–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏. –ö–∞–∫–æ–π —Ç–∏–ø, —Ü–≤–µ—Ç, —Å—Ç–∏–ª—å –∏ –º–∞—Ç–µ—Ä–∏–∞–ª? –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ—á–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –º–æ–¥—ã.'
        app.logger.warning(f"‚ö†Ô∏è –§–∞–π–ª –ø—Ä–æ–º–ø—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω: {prompt_file}. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ–º–ø—Ç –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
        print(f"‚ö†Ô∏è –§–∞–π–ª –ø—Ä–æ–º–ø—Ç–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ–º–ø—Ç –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")

    except Exception as e:
        default_prompt = '–û–ø–∏—à–∏ –ø–æ–¥—Ä–æ–±–Ω–æ –∫–∞–∫–∏–µ –ø—Ä–µ–¥–º–µ—Ç—ã –æ–¥–µ–∂–¥—ã —Ç—ã –≤–∏–¥–∏—à—å –Ω–∞ —ç—Ç–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏. –ö–∞–∫–æ–π —Ç–∏–ø, —Ü–≤–µ—Ç, —Å—Ç–∏–ª—å –∏ –º–∞—Ç–µ—Ä–∏–∞–ª? –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ—á–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –º–æ–¥—ã.'
        app.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–æ–º–ø—Ç–∞: {e}. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ–º–ø—Ç –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–æ–º–ø—Ç–∞. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–æ–º–ø—Ç –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é")

def load_model():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç FastVLM –º–æ–¥–µ–ª—å –≤ –ø–∞–º—è—Ç—å"""
    global model, tokenizer, image_processor, context_len

    try:
        print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º FastVLM –º–æ–¥–µ–ª—å –≤ –ø–∞–º—è—Ç—å...")
        app.logger.info("–ù–∞—á–∞–ª–æ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        if not os.path.exists(Config.MODEL_PATH):
            raise FileNotFoundError(f"–ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞: {Config.MODEL_PATH}")

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        disable_torch_init()
        model_name = get_model_name_from_path(Config.MODEL_PATH)
        tokenizer, model, image_processor, context_len = load_pretrained_model(
            Config.MODEL_PATH, None, model_name,
            device=Config.DEVICE,
            torch_dtype=Config.TORCH_DTYPE
        )

        app.logger.info(f"FastVLM –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞: {model_name} –Ω–∞ {Config.DEVICE}")
        print("FastVLM –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!")
        return True

    except Exception as e:
        error_msg = f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}"
        print(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        app.logger.error(error_msg, exc_info=True)
        return False

def extract_analysis_from_output(output):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∞–Ω–∞–ª–∏–∑–∞ –∏–∑ –≤—ã–≤–æ–¥–∞ FastVLM"""
    try:
        lines = output.strip().split('\n')

        # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å—Ç—Ä–æ–∫—É —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º
        result_lines = []
        for line in reversed(lines):
            line = line.strip()
            if line and not line.startswith('`torch_dtype`') and not line.startswith('The following'):
                # –û—á–∏—â–∞–µ–º –æ—Ç –º—É—Å–æ—Ä–∞
                clean_line = line.encode('utf-8', errors='replace').decode('utf-8', errors='replace')
                clean_line = ' '.join(clean_line.split())
                result_lines.insert(0, clean_line)

        result_text = '\n'.join(result_lines[:10])

        if not result_text:
            result_text = output.strip()

        return result_text

    except Exception as e:
        app.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∞–Ω–∞–ª–∏–∑–∞: {e}")
        return "–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞"

@app.route('/health', methods=['GET'])
def health():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–µ—Ä–∞"""
    try:
        health_data = {
            'status': 'healthy',
            'model_loaded': model is not None,
            'timestamp': time.time(),
            'device': Config.DEVICE,
            'torch_version': torch.__version__
        }

        app.logger.debug("Health check requested")
        return jsonify(health_data)

    except Exception as e:
        app.logger.error(f"Health check error: {e}")
        return jsonify({
            'status': 'error',
            'error': str(e),
            'timestamp': time.time()
        }), 500

@app.route('/analyze', methods=['POST'])
def analyze():
    """–ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    try:
        if model is None:
            return jsonify({
                'success': False,
                'error': 'Model not loaded'
            }), 500

        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        data = request.get_json()
        if not data or 'image_base64' not in data:
            return jsonify({
                'success': False,
                'error': 'No image provided'
            }), 400

        image_base64 = data['image_base64']
        prompt = data.get('prompt', default_prompt)

        app.logger.info("–ù–∞—á–∞–ª–æ –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")

        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        try:
            image_data = base64.b64decode(image_base64)
            image = Image.open(io.BytesIO(image_data))
        except Exception as e:
            app.logger.error(f"–û—à–∏–±–∫–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
            return jsonify({
                'success': False,
                'error': f'Invalid image data: {e}'
            }), 400

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as temp_file:
            image.save(temp_file, 'JPEG')
            temp_image_path = temp_file.name

        try:
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –º–æ–¥–µ–ª–∏
            qs = prompt
            if model.config.mm_use_im_start_end:
                qs = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\n' + qs
            else:
                qs = DEFAULT_IMAGE_TOKEN + '\n' + qs

            conv = conv_templates["qwen_2"].copy()
            conv.append_message(conv.roles[0], qs)
            conv.append_message(conv.roles[1], None)
            prompt_full = conv.get_prompt()

            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
            input_ids = tokenizer_image_token(prompt_full, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(model.device)

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image_tensor = process_images([image], image_processor, model.config)[0]

            # –í—ã–ø–æ–ª–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑
            with torch.no_grad():
                output_ids = model.generate(
                    input_ids,
                    images=image_tensor.unsqueeze(0).to(model.device).half(),
                    image_sizes=[image.size],
                    do_sample=Config.DO_SAMPLE,
                    temperature=Config.TEMPERATURE,
                    top_p=None,
                    num_beams=1,
                    max_new_tokens=Config.MAX_NEW_TOKENS,
                    use_cache=True
                )

            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result_text = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å—Ç—ã–π –∞–Ω–∞–ª–∏–∑
            clean_analysis = extract_analysis_from_output(result_text)

            app.logger.info("–ê–Ω–∞–ª–∏–∑ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω")

            return jsonify({
                'success': True,
                'analysis': clean_analysis,
                'model_used': model.config.model_type,
                'device': str(model.device)
            })

        finally:
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            try:
                os.unlink(temp_image_path)
            except:
                pass

    except Exception as e:
        error_msg = f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}"
        app.logger.error(error_msg, exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.route('/load', methods=['GET'])
def get_load():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–≥—Ä—É–∑–∫–∏ —Å–µ—Ä–≤–µ—Ä–∞"""
    try:
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()

        load_data = {
            'cpu_percent': cpu_percent,
            'memory_percent': memory.percent,
            'memory_used_gb': round(memory.used / (1024**3), 2),
            'memory_total_gb': round(memory.total / (1024**3), 2),
            'timestamp': time.time()
        }

        app.logger.debug(f"Load check: CPU {cpu_percent}%, Memory {memory.percent}%")
        return jsonify(load_data)

    except Exception as e:
        app.logger.error(f"Load check error: {e}")
        return jsonify({
            'error': str(e),
            'timestamp': time.time()
        }), 500

@app.route('/gpu', methods=['GET'])
def get_gpu_info():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–∞–±–æ—Ç—ã –Ω–∞ GPU"""
    try:
        if not torch.cuda.is_available():
            return jsonify({
                'gpu_available': False,
                'message': 'GPU –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω',
                'device': 'cpu'
            })

        gpu_info = {
            'gpu_available': True,
            'gpu_name': torch.cuda.get_device_name(0),
            'gpu_memory_allocated_mb': round(torch.cuda.memory_allocated(0) / (1024**2), 2),
            'gpu_memory_reserved_mb': round(torch.cuda.memory_reserved(0) / (1024**2), 2),
            'gpu_memory_total_mb': round(torch.cuda.get_device_properties(0).total_memory / (1024**2), 2),
            'device': 'cuda'
        }

        app.logger.debug(f"GPU info: {gpu_info['gpu_name']}")
        return jsonify(gpu_info)

    except Exception as e:
        app.logger.error(f"GPU check error: {e}")
        return jsonify({
            'gpu_available': False,
            'error': str(e),
            'device': 'cpu'
        }), 500

@app.route('/model', methods=['GET'])
def get_model_info():
    """–ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏"""
    try:
        if model is None:
            return jsonify({
                'loaded': False,
                'message': '–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞'
            })

        model_info = {
            'loaded': True,
            'model_name': model.config.model_type,
            'device': str(model.device),
            'context_length': context_len,
            'torch_dtype': str(Config.TORCH_DTYPE),
            'model_path': Config.MODEL_PATH
        }

        app.logger.debug(f"Model info: {model_info['model_name']}")
        return jsonify(model_info)

    except Exception as e:
        app.logger.error(f"Model info error: {e}")
        return jsonify({
            'loaded': False,
            'error': str(e)
        }), 500

def signal_handler(signum, frame):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∏–≥–Ω–∞–ª–æ–≤ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è"""
    print("–ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è, –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —Å–µ—Ä–≤–µ—Ä...")
    app.logger.info("Server shutdown initiated")

    if model and torch.cuda.is_available():
        # –û—á–∏—Å—Ç–∫–∞ GPU –ø–∞–º—è—Ç–∏
        torch.cuda.empty_cache()
        app.logger.info("GPU memory cleared")

    sys.exit(0)

def start_server():
    """–ó–∞–ø—É—Å–∫ Flask —Å–µ—Ä–≤–µ—Ä–∞"""
    try:
        print(f"üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º FastVLM —Å–µ—Ä–≤–µ—Ä –Ω–∞ {Config.HOST}:{Config.PORT}...")
        app.logger.info(f"Server starting on {Config.HOST}:{Config.PORT}")

        app.run(
            host=Config.HOST,
            port=Config.PORT,
            debug=False,
            use_reloader=False
        )
    except Exception as e:
        error_msg = f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ FastVLM —Å–µ—Ä–≤–µ—Ä–∞: {e}"
        print(error_msg)
        app.logger.error(error_msg, exc_info=True)

if __name__ == '__main__':
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
    Config.load_env()

    # –°–æ–∑–¥–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    Config.ensure_directories()

    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
    setup_logging()

    print("FastVLM Server starting...")

    # –í–∞–ª–∏–¥–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    try:
        Config.validate_config()
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
        sys.exit(1)

    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ–±—Ä–∞–±–æ—Ç—á–∏–∫–∏ —Å–∏–≥–Ω–∞–ª–æ–≤
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ–º–ø—Ç
    load_prompt()

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    if load_model():
        # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–µ—Ä–≤–µ—Ä
        start_server()
    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å, —Å–µ—Ä–≤–µ—Ä –Ω–µ –∑–∞–ø—É—â–µ–Ω")
        sys.exit(1)
