#!/usr/bin/env python3
"""
FastVLM Server - –ø–æ—Å—Ç–æ—è–Ω–Ω—ã–π —Å–µ—Ä–≤–µ—Ä –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
–ó–∞–ø—É—Å–∫–∞–µ—Ç—Å—è –æ–¥–∏–Ω —Ä–∞–∑ –∏ –¥–µ—Ä–∂–∏—Ç –º–æ–¥–µ–ª—å –≤ –ø–∞–º—è—Ç–∏
"""

import sys
import json
import base64
import tempfile
import threading
import time
from flask import Flask, request, jsonify
from PIL import Image
import io
import os

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –º–æ–¥—É–ª–∏
import torch

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º FastVLM
sys.path.append('../../../ml-fastvlm')
from llava.utils import disable_torch_init
from llava.conversation import conv_templates
from llava.model.builder import load_pretrained_model
from llava.mm_utils import tokenizer_image_token, process_images, get_model_name_from_path
from llava.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN, DEFAULT_IM_START_TOKEN, DEFAULT_IM_END_TOKEN

app = Flask(__name__)

# –ì–ª–æ–±–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –¥–ª—è –º–æ–¥–µ–ª–∏
model = None
tokenizer = None
image_processor = None
context_len = None

def load_model():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç FastVLM –º–æ–¥–µ–ª—å –≤ –ø–∞–º—è—Ç—å"""
    global model, tokenizer, image_processor, context_len

    try:
        print("üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º FastVLM –º–æ–¥–µ–ª—å –≤ –ø–∞–º—è—Ç—å...")

        # –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏
        current_dir = os.path.dirname(os.path.abspath(__file__))
        model_path = os.path.join(current_dir, '../../../ml-fastvlm/checkpoints/llava-fastvithd_1.5b_stage3')

        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        disable_torch_init()
        model_name = get_model_name_from_path(model_path)
        tokenizer, model, image_processor, context_len = load_pretrained_model(
            model_path, None, model_name, device="cuda" if torch.cuda.is_available() else "cpu", torch_dtype=torch.float16
        )

        print("‚úÖ FastVLM –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!")
        return True

    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}")
        return False

def extract_analysis_from_output(output):
    """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Ç–µ–∫—Å—Ç –∞–Ω–∞–ª–∏–∑–∞ –∏–∑ –≤—ã–≤–æ–¥–∞ FastVLM"""
    try:
        lines = output.strip().split('\n')

        # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω—é—é —Å—Ç—Ä–æ–∫—É —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º
        result_lines = []
        for line in reversed(lines):
            line = line.strip()
            if line and not line.startswith('`torch_dtype`') and not line.startswith('The following'):
                # –û—á–∏—â–∞–µ–º –æ—Ç –º—É—Å–æ—Ä–∞
                clean_line = line.encode('utf-8', errors='replace').decode('utf-8', errors='replace')
                clean_line = ' '.join(clean_line.split())
                result_lines.insert(0, clean_line)

        result_text = '\n'.join(result_lines[:10])

        if not result_text:
            result_text = output.strip()

        return result_text

    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∞–Ω–∞–ª–∏–∑–∞: {e}")
        return "–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞"

@app.route('/health', methods=['GET'])
def health():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–¥–æ—Ä–æ–≤—å—è —Å–µ—Ä–≤–µ—Ä–∞"""
    return jsonify({
        'status': 'healthy',
        'model_loaded': model is not None,
        'timestamp': time.time()
    })

@app.route('/analyze', methods=['POST'])
def analyze():
    """–ê–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    try:
        if model is None:
            return jsonify({
                'success': False,
                'error': 'Model not loaded'
            }), 500

        # –ü–æ–ª—É—á–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        data = request.get_json()
        if not data or 'image_base64' not in data:
            return jsonify({
                'success': False,
                'error': 'No image provided'
            }), 400

        image_base64 = data['image_base64']
        prompt = data.get('prompt', '–û–ø–∏—à–∏ –ø–æ–¥—Ä–æ–±–Ω–æ –∫–∞–∫–∏–µ –ø—Ä–µ–¥–º–µ—Ç—ã –æ–¥–µ–∂–¥—ã —Ç—ã –≤–∏–¥–∏—à—å –Ω–∞ —ç—Ç–æ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏. –ö–∞–∫–æ–π —Ç–∏–ø, —Ü–≤–µ—Ç, —Å—Ç–∏–ª—å –∏ –º–∞—Ç–µ—Ä–∏–∞–ª? –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ—á–Ω—ã–µ —Ç–µ—Ä–º–∏–Ω—ã –º–æ–¥—ã.')

        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        image_data = base64.b64decode(image_base64)
        image = Image.open(io.BytesIO(image_data))

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤–æ –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
        with tempfile.NamedTemporaryFile(suffix='.jpg', delete=False) as temp_file:
            image.save(temp_file, 'JPEG')
            temp_image_path = temp_file.name

        try:
            # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ–º–ø—Ç –¥–ª—è –º–æ–¥–µ–ª–∏
            qs = prompt
            if model.config.mm_use_im_start_end:
                qs = DEFAULT_IM_START_TOKEN + DEFAULT_IMAGE_TOKEN + DEFAULT_IM_END_TOKEN + '\n' + qs
            else:
                qs = DEFAULT_IMAGE_TOKEN + '\n' + qs

            conv = conv_templates["qwen_2"].copy()
            conv.append_message(conv.roles[0], qs)
            conv.append_message(conv.roles[1], None)
            prompt_full = conv.get_prompt()

            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
            input_ids = tokenizer_image_token(prompt_full, tokenizer, IMAGE_TOKEN_INDEX, return_tensors='pt').unsqueeze(0).to(model.device)

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image_tensor = process_images([image], image_processor, model.config)[0]

            # –í—ã–ø–æ–ª–Ω—è–µ–º –∞–Ω–∞–ª–∏–∑
            with torch.no_grad():
                output_ids = model.generate(
                    input_ids,
                    images=image_tensor.unsqueeze(0).to(model.device).half(),
                    image_sizes=[image.size],
                    do_sample=True,
                    temperature=0.2,
                    top_p=None,
                    num_beams=1,
                    max_new_tokens=256,
                    use_cache=True
                )

            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
            result_text = tokenizer.batch_decode(output_ids, skip_special_tokens=True)[0].strip()

            # –ò–∑–≤–ª–µ–∫–∞–µ–º —á–∏—Å—Ç—ã–π –∞–Ω–∞–ª–∏–∑
            clean_analysis = extract_analysis_from_output(result_text)

            return jsonify({
                'success': True,
                'analysis': clean_analysis
            })

        finally:
            # –£–¥–∞–ª—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            try:
                os.unlink(temp_image_path)
            except:
                pass

    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞: {e}")
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

def start_server():
    """–ó–∞–ø—É—Å–∫ Flask —Å–µ—Ä–≤–µ—Ä–∞ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ"""
    try:
        print("üöÄ –ó–∞–ø—É—Å–∫–∞–µ–º FastVLM —Å–µ—Ä–≤–µ—Ä –Ω–∞ –ø–æ—Ä—Ç—É 3001...")
        app.run(host='127.0.0.1', port=3001, debug=False, use_reloader=False)
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ FastVLM —Å–µ—Ä–≤–µ—Ä–∞: {e}")

if __name__ == '__main__':
    import torch

    print("ü§ñ FastVLM Server starting...")

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    if load_model():
        # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–µ—Ä–≤–µ—Ä
        start_server()
    else:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å, —Å–µ—Ä–≤–µ—Ä –Ω–µ –∑–∞–ø—É—â–µ–Ω")
        sys.exit(1)
